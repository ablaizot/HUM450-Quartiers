{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13887\n",
      "13847\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "geom = gpd.read_file('src/Vectorisation/')\n",
    "geom = geom.reset_index()\n",
    "geom.rename(columns={'index': 'geom_id'}, inplace=True)\n",
    "df = pd.read_excel('src/legende_renove_2.xlsx')\n",
    "# only one entry with no folio, we can drop it (only information is in column \"*\": 'MANQUE DES NUMEROS POUR FO. 10 (IL MANQUE PROBABLEMENT UNE PAGE)')\n",
    "df = df[df['folio'].notnull()]\n",
    "points = gpd.read_file('src/numeros_merged-v2.json') # no null values in the relevant part. \n",
    "print(len(points))\n",
    "points.drop(columns=['id'], inplace=True)\n",
    "points = points.drop_duplicates()\n",
    "print(len(points))\n",
    "\n",
    "def check_decimal_vals(df, col) -> None:\n",
    "    tdf = df.copy()\n",
    "    tdf['decimal_part'] = tdf.apply(lambda x: x[col] - int(x[col]), axis=1)\n",
    "    dec_vals = list(tdf.decimal_part.value_counts().items())\n",
    "    if len(dec_vals) > 1 or (len(dec_vals) == 1 and dec_vals[0][0] != 0.0):\n",
    "        print(f'There are {col} with decimal parts. This is unexpected.')\n",
    "        print(dec_vals)\n",
    "        print('Please check the data.')\n",
    "\n",
    "def number_to_parcel_id(number: float) -> str:\n",
    "    str_number = str(number)\n",
    "    if '.' not in str_number:\n",
    "        return str_number\n",
    "    vals = [v.strip() for v in str(number).split('.')]\n",
    "    if len(vals) == 1:\n",
    "        print(f'Unexpected value: {number}.')\n",
    "        return vals[0]\n",
    "    main_part = vals[0]\n",
    "    decim_part = vals[1]\n",
    "    if decim_part == '0':\n",
    "        return main_part\n",
    "    return f'{main_part}-{decim_part}'\n",
    "\n",
    "def fix_numerical_error(v:float) -> str:\n",
    "    return str(float(int(10*float(v)))/(10.))\n",
    "\n",
    "check_decimal_vals(df, 'folio')\n",
    "check_decimal_vals(points, 'folio')\n",
    "df['parcel_id'] = df['nr'].apply(number_to_parcel_id)\n",
    "df['folio'] = df['folio'].astype(int).astype(str)\n",
    "points['folio'] = points['folio'].astype(int).astype(str)\n",
    "points['num'] = points['num'].apply(fix_numerical_error) # fix numerical error\n",
    "points['parcel_id'] = points['num'].apply(number_to_parcel_id)\n",
    "df['merge_id'] = df['folio'] + 'f' + df['parcel_id']\n",
    "points['merge_id'] = points['folio'] + 'f' + points['parcel_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db38454350c44de9476cad5675e187f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def locate_parcel_geom_id(geom: list[tuple[str, Polygon]], point: Point) -> str:\n",
    "    for id, geometry in geom:\n",
    "        if geometry.contains(point):\n",
    "            return id\n",
    "    return None\n",
    "\n",
    "geom_dict = list(geom.set_index('geom_id')['geometry'].to_dict().items())\n",
    "geom_dict\n",
    "\n",
    "# this part is slow, around 25 to 30 minutes. Optimisation possible according to that link, but require numba: https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python\n",
    "points['geom_id'] =  points.progress_apply(lambda x: locate_parcel_geom_id(geom_dict, x.geometry), axis=1)\n",
    "points['geom_id'] = points['geom_id'].apply(lambda x: str(int(x)) if not pd.isna(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we don't need to recompute the geometry and to which polygon they belong. \n",
    "points.to_crs('EPSG:4326').to_file('src/numeros_merged-v3.geojson', driver='GeoJSON')\n",
    "geom.to_crs('EPSG:4326').to_file('src/geometries_with_index.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = gpd.read_file('numeros_merged-v3.geojson')\n",
    "num_df.drop(columns=['id'], inplace=True)\n",
    "num_df = num_df.drop_duplicates()\n",
    "gdf = gpd.read_file('geometries_with_index.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prove RÃ©mi's claim that the lowest page_number and folio_nmbr is always the first to appear in the lowest volume in the cadastral planches\n",
    "check_number_order = False\n",
    "if check_number_order:\n",
    "    gdf['pg_nbr'] = gdf['layer'].apply(lambda s: s.split('_')[-1] if s else None)\n",
    "    gdf['folio_nbr'] = gdf['layer'].apply(lambda s: s.split('_')[-2] if s else None)\n",
    "    gdf[['folio_nbr', 'pg_nbr']].drop_duplicates().sort_values(by=['folio_nbr', 'pg_nbr']).iloc[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=df.merge(num_df[['merge_id', 'geom_id', 'geometry']], on='merge_id', how='left')\n",
    "merged_df['geometries'] = merged_df.apply(lambda v: [v['geom_id'], v['geometry']] ,axis=1)\n",
    "df_final = df.set_index('*')\n",
    "df_final['geometries'] = merged_df[['*', 'geometries']].groupby('*').agg(list).reset_index().set_index('*')\n",
    "df_final['geometries'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['geom_id'] = gdf['geom_id'].astype(str)\n",
    "if 'pg_nbr' in gdf.columns:\n",
    "    gdf.drop(columns=['pg_nbr'], inplace=True)\n",
    "if 'folio_nbr' in gdf.columns:\n",
    "    gdf.drop(columns=['folio_nbr'], inplace=True)\n",
    "gdf.to_file('lausanne-1888-cadastre-renove-geometries-20250311.geojson', driver='GeoJSON')\n",
    "df_final.drop(columns=['merge_id', 'nr']).to_csv('lausanne-1888-cadastre-renove-registre-20250311.csv')\n",
    "num_df.drop(columns=['num']).to_file('lausanne-1888-cadastre-renove-points-20250311.geojson', driver='GeoJSON')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
